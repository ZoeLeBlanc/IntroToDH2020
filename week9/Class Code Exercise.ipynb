{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "alt.renderers.enable('notebook')\n",
    "alt.data_transformers.enable('default', max_rows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('humanist_vols_ner_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test out TF-IDF vs Word Counts vs NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = df.cleaned_text.tolist()\n",
    "\n",
    "# Specify the ngram_range for the Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.5, min_df=1)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Output the top tokens for each document\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    print(one_doc_as_df[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df.explode('cleaned_tokens')\n",
    "df_grouped = df_exploded.groupby(['dates', 'cleaned_tokens']).size().reset_index(name='frequency')\n",
    "df_grouped.sort_values(by=['dates', 'frequency' ], inplace=True, ascending=False)\n",
    "df_top = df_grouped.sort_values(by=['dates','frequency'],ascending = False).groupby('dates').head(10)\n",
    "df_pivot = df_top.pivot(index='cleaned_tokens', columns='dates', values='frequency').fillna(0)\n",
    "df_top = df_pivot.unstack().reset_index(name='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_top).mark_area(point=True).encode(\n",
    "    y='frequency:Q',\n",
    "    x='dates',\n",
    "    color='cleaned_tokens:N',\n",
    "    tooltip=['cleaned_tokens', 'frequency', 'dates'],\n",
    "    row='cleaned_tokens:N'\n",
    ").properties(height=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ner_tokens']= df.ner.apply(nltk.word_tokenize)\n",
    "df['ner_counts'] = df.ner_tokens.str.len()\n",
    "df['year'] = pd.to_datetime(df.dates.str[0:4], format='%Y')\n",
    "alt.Chart(df[['year', 'ner_counts']]).mark_bar().encode(\n",
    "    x='year:T',\n",
    "    y='ner_counts:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df[['original_text_length', 'ner_counts', 'year']]).mark_point().encode(\n",
    "    x='original_text_length:Q',\n",
    "    y='ner_counts:Q',\n",
    "    color='year:T'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest = alt.selection(type='single', nearest=True, on='mouseover',\n",
    "#                         fields=['year'], empty='none')\n",
    "# line = alt.Chart(df_top).mark_line().encode(\n",
    "#     y='frequency:Q',\n",
    "#     x='year:T',\n",
    "#     color=alt.Color('ner_tokens:N', scale=alt.Scale(scheme='category20b')),\n",
    "# )\n",
    "# selectors = alt.Chart(df_top).mark_point().encode(\n",
    "#     x='year:T',\n",
    "#     opacity=alt.value(0),\n",
    "# ).add_selection(\n",
    "#     nearest\n",
    "# )\n",
    "# points = line.mark_point().encode(\n",
    "#     opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "# )\n",
    "# rules = alt.Chart(df_top).mark_rule(color='gray').encode(\n",
    "#     x='year:T',\n",
    "# ).transform_filter(\n",
    "#     nearest\n",
    "# )\n",
    "# text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "#     text=alt.condition(nearest, 'frequency:Q', alt.value(' '))\n",
    "# )\n",
    "\n",
    "# alt.layer(\n",
    "#     line, selectors, points, rules, text\n",
    "# ).properties(\n",
    "#     width=600, height=500\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week9",
   "language": "python",
   "name": "week9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
